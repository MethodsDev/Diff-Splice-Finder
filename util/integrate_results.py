#!/usr/bin/env python3

"""
Integrate results from donor and acceptor clustering analyses.

Creates unified summary files combining both analyses, allowing users to:
- See all tested introns across both clustering strategies
- Identify introns significant in donor, acceptor, or both
- Compare effect sizes between clustering methods
- Annotate introns with gene information from GTF (optional)
"""

import sys
import os
import argparse
import logging
import pandas as pd
import numpy as np
from collections import defaultdict
import re
import time

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s : %(levelname)s : %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger(__name__)


def parse_intron_id(intron_id):
    """
    Parse intron ID to extract coordinates.
    
    Supports multiple formats:
    - chr:start-end:strand
    - chr:start-end^MOTIF^STATUS (extracts strand from motif)
    
    Args:
        intron_id: Intron identifier string
        
    Returns:
        Tuple of (chr, start, end, strand) or None if parsing fails
    """
    try:
        # Format 1: chr:start-end:strand
        match = re.match(r'^(.+):(\d+)-(\d+):([+-])$', intron_id)
        if match:
            chrom, start, end, strand = match.groups()
            return chrom, int(start), int(end), strand
        
        # Format 2: chr:start-end^MOTIF^STATUS
        # Motifs: GT--AG (plus strand), CT--AC (minus strand)
        match = re.match(r'^(.+):(\d+)-(\d+)\^([ACGT]{2})--([ACGT]{2})\^', intron_id)
        if match:
            chrom, start, end, donor, acceptor = match.groups()
            # Determine strand from splice motif
            # GT--AG is canonical for + strand
            # CT--AC is canonical for - strand (reverse complement of GT--AG)
            if donor == 'GT' and acceptor == 'AG':
                strand = '+'
            elif donor == 'CT' and acceptor == 'AC':
                strand = '-'
            else:
                # Non-canonical, try to infer or use +
                strand = '+' if donor == 'GT' else '-' if donor == 'CT' else '+'
            
            return chrom, int(start), int(end), strand
            
    except Exception as e:
        logger.warning(f"Failed to parse intron_id '{intron_id}': {e}")
    return None


def get_cached_gtf_filename(gtf_file):
    """
    Generate the cached intron annotation filename from GTF path.
    
    Args:
        gtf_file: Path to GTF annotation file
        
    Returns:
        Path to cached annotation file
    """
    # Create cache filename in same directory as GTF
    base = os.path.splitext(gtf_file)[0]
    return f"{base}.intron_cache.tsv"


def write_gtf_cache(cache_file, gene_map, annotated_introns, transcript_genes, transcript_exons):
    """
    Write parsed GTF data to cache file for fast re-loading.
    
    Args:
        cache_file: Path to cache file
        gene_map: Dict of gene coordinates to gene info
        annotated_introns: Set of annotated intron coordinates
        transcript_genes: Dict mapping transcript_id to gene info
        transcript_exons: Dict mapping transcript_id to exon list
    """
    logger.info(f"Writing GTF cache to: {cache_file}")
    
    with open(cache_file, 'w') as f:
        # Write header
        f.write("## GTF Intron Cache File\n")
        f.write("## Generated by Diff-Splice-Finder\n")
        f.write(f"## Date: {pd.Timestamp.now()}\n")
        f.write("##\n")
        
        # Section 1: Annotated introns with gene assignments
        f.write("## Annotated Introns\n")
        f.write("chr\tstart\tend\tstrand\tgene_names\n")
        
        # For each intron, find all genes it belongs to
        intron_genes = defaultdict(set)
        for transcript_id, exons in transcript_exons.items():
            if len(exons) < 2:
                continue
            
            gene_info = transcript_genes.get(transcript_id)
            if not gene_info:
                continue
                
            gene_name = gene_info['gene_name']
            exons_sorted = sorted(exons, key=lambda x: x[1])
            
            # Get introns for this transcript
            for i in range(len(exons_sorted) - 1):
                chrom1, start1, end1, strand = exons_sorted[i]
                chrom2, start2, end2, _ = exons_sorted[i + 1]
                
                if chrom1 == chrom2:
                    intron_start = end1 + 1
                    intron_end = start2 - 1
                    
                    if intron_start < intron_end:
                        intron_key = (chrom1, intron_start, intron_end, strand)
                        intron_genes[intron_key].add(gene_name)
        
        # Write introns
        for intron_coords in sorted(annotated_introns):
            chrom, start, end, strand = intron_coords
            genes = sorted(intron_genes.get(intron_coords, set()))
            gene_str = ','.join(genes) if genes else '.'
            f.write(f"{chrom}\t{start}\t{end}\t{strand}\t{gene_str}\n")
        
        # Section 2: Gene regions for overlap checking
        f.write("##\n")
        f.write("## Gene Regions\n")
        f.write("chr\tstart\tend\tstrand\tgene_id\tgene_name\n")
        
        for gene_coords, gene_info in sorted(gene_map.items()):
            chrom, start, end, strand = gene_coords
            f.write(f"{chrom}\t{start}\t{end}\t{strand}\t{gene_info['gene_id']}\t{gene_info['gene_name']}\n")
    
    logger.info(f"Cached {len(annotated_introns)} introns and {len(gene_map)} genes")


def read_gtf_cache(cache_file):
    """
    Read cached GTF data from file.
    
    Args:
        cache_file: Path to cache file
        
    Returns:
        Tuple of (gene_map, annotated_introns, intron_gene_map):
        - gene_map: Dict mapping (chr, start, end, strand) to gene info
        - annotated_introns: Set of (chr, start, end, strand) tuples
        - intron_gene_map: Dict mapping intron coords to set of gene names
    """
    logger.info(f"Reading GTF cache from: {cache_file}")
    
    annotated_introns = set()
    intron_gene_map = {}
    gene_map = {}
    
    section = None
    
    with open(cache_file, 'r') as f:
        for line in f:
            line = line.strip()
            
            if not line or line.startswith('##'):
                if 'Annotated Introns' in line:
                    section = 'introns'
                elif 'Gene Regions' in line:
                    section = 'genes'
                continue
            
            # Skip headers
            if line.startswith('chr\t'):
                continue
            
            fields = line.split('\t')
            
            if section == 'introns' and len(fields) >= 5:
                chrom, start, end, strand, gene_str = fields[0], int(fields[1]), int(fields[2]), fields[3], fields[4]
                intron_coords = (chrom, start, end, strand)
                annotated_introns.add(intron_coords)
                
                if gene_str != '.':
                    genes = set(gene_str.split(','))
                    intron_gene_map[intron_coords] = genes
            
            elif section == 'genes' and len(fields) >= 6:
                chrom, start, end, strand, gene_id, gene_name = fields
                gene_coords = (chrom, int(start), int(end), strand)
                gene_map[gene_coords] = {
                    'gene_id': gene_id,
                    'gene_name': gene_name,
                    'chr': chrom,
                    'start': int(start),
                    'end': int(end),
                    'strand': strand
                }
    
    logger.info(f"Loaded {len(annotated_introns)} introns and {len(gene_map)} genes from cache")
    
    return gene_map, annotated_introns, intron_gene_map


def parse_gtf_file(gtf_file):
    """
    Parse GTF file to extract gene information and annotated introns.
    Uses cached data if available, otherwise parses GTF and creates cache.
    
    Args:
        gtf_file: Path to GTF annotation file
        
    Returns:
        Tuple of (gene_map, annotated_introns, transcript_genes, transcript_exons):
        - gene_map: Dict mapping (chr, start, end, strand) to gene info
        - annotated_introns: Set of (chr, start, end, strand) tuples for known introns
        - transcript_genes: Dict mapping transcript_id to gene info (empty if from cache)
        - transcript_exons: Dict mapping transcript_id to exon list (empty if from cache)
    """
    # Check for cached version
    cache_file = get_cached_gtf_filename(gtf_file)
    
    if os.path.exists(cache_file):
        # Check if cache is newer than GTF
        gtf_mtime = os.path.getmtime(gtf_file)
        cache_mtime = os.path.getmtime(cache_file)
        
        if cache_mtime >= gtf_mtime:
            logger.info(f"Using cached GTF data: {cache_file}")
            gene_map, annotated_introns, intron_gene_map = read_gtf_cache(cache_file)
            
            # For cached data, we return empty dicts for transcript data
            # but store intron->gene mapping for later use
            transcript_genes = {}
            transcript_exons = {}
            
            # Store the intron_gene_map for use in find_overlapping_genes
            return gene_map, annotated_introns, transcript_genes, transcript_exons, intron_gene_map
        else:
            logger.info(f"Cache file outdated, re-parsing GTF")
    
    logger.info(f"Parsing GTF file: {gtf_file}")
    
    gene_map = {}  # Maps coordinates to gene info
    transcript_exons = defaultdict(list)  # Maps transcript_id to list of exons
    transcript_genes = {}  # Maps transcript_id to gene info
    
    with open(gtf_file, 'r') as f:
        for line in f:
            if line.startswith('#'):
                continue
                
            fields = line.strip().split('\t')
            if len(fields) < 9:
                continue
                
            chrom = fields[0]
            feature_type = fields[2]
            start = int(fields[3])
            end = int(fields[4])
            strand = fields[6]
            attributes = fields[8]
            
            # Parse attributes
            attr_dict = {}
            for attr in attributes.split(';'):
                attr = attr.strip()
                if attr:
                    match = re.match(r'(\S+)\s+"([^"]+)"', attr)
                    if match:
                        attr_dict[match.group(1)] = match.group(2)
            
            gene_id = attr_dict.get('gene_id', '')
            gene_name = attr_dict.get('gene_name', gene_id)
            transcript_id = attr_dict.get('transcript_id', '')
            
            # Store gene information for exons
            if feature_type == 'exon' and transcript_id:
                transcript_exons[transcript_id].append((chrom, start, end, strand))
                if transcript_id not in transcript_genes:
                    transcript_genes[transcript_id] = {
                        'gene_id': gene_id,
                        'gene_name': gene_name,
                        'chr': chrom,
                        'strand': strand
                    }
                    
            # Also store gene features for overlap checking
            elif feature_type == 'gene':
                gene_key = (chrom, start, end, strand)
                gene_map[gene_key] = {
                    'gene_id': gene_id,
                    'gene_name': gene_name,
                    'chr': chrom,
                    'start': start,
                    'end': end,
                    'strand': strand
                }
    
    # Extract introns from exons and build intron->gene mapping
    annotated_introns = set()
    intron_gene_map = defaultdict(set)
    
    for transcript_id, exons in transcript_exons.items():
        if len(exons) < 2:
            continue
        
        gene_info = transcript_genes.get(transcript_id)
        if not gene_info:
            continue
        
        gene_name = gene_info['gene_name']
        
        # Sort exons by start position
        exons = sorted(exons, key=lambda x: x[1])
        
        # Extract introns between consecutive exons
        for i in range(len(exons) - 1):
            chrom1, start1, end1, strand = exons[i]
            chrom2, start2, end2, _ = exons[i + 1]
            
            if chrom1 == chrom2:  # Same chromosome
                # Intron coordinates: end of exon i + 1 to start of exon i+1 - 1
                intron_start = end1 + 1
                intron_end = start2 - 1
                
                if intron_start < intron_end:  # Valid intron
                    intron_coords = (chrom1, intron_start, intron_end, strand)
                    annotated_introns.add(intron_coords)
                    intron_gene_map[intron_coords].add(gene_name)
    
    logger.info(f"Parsed {len(gene_map)} genes and {len(annotated_introns)} annotated introns")
    
    # Write cache file
    try:
        write_gtf_cache(cache_file, gene_map, annotated_introns, transcript_genes, transcript_exons)
    except Exception as e:
        logger.warning(f"Failed to write cache file: {e}")
    
    # Return the intron_gene_map for fast known-intron lookups
    return gene_map, annotated_introns, transcript_genes, transcript_exons, intron_gene_map


def build_gene_index(gene_map, transcript_genes, transcript_exons):
    """
    Build spatial index for fast gene lookups by chromosome and strand.
    
    Args:
        gene_map: Dict of gene coordinates to gene info
        transcript_genes: Dict mapping transcript_id to gene info
        transcript_exons: Dict mapping transcript_id to exon list
        
    Returns:
        Dict mapping (chr, strand) to list of (start, end, gene_name) tuples
    """
    logger.info("Building spatial index for genes...")
    
    index = defaultdict(list)
    
    # Index genes
    for gene_coords, gene_info in gene_map.items():
        chrom, start, end, strand = gene_coords
        key = (chrom, strand)
        index[key].append((start, end, gene_info['gene_name']))
    
    # Index transcripts (get their spans)
    for transcript_id, exons in transcript_exons.items():
        if not exons:
            continue
            
        gene_info = transcript_genes.get(transcript_id)
        if not gene_info:
            continue
        
        exons_sorted = sorted(exons, key=lambda x: x[1])
        transcript_start = exons_sorted[0][1]
        transcript_end = exons_sorted[-1][2]
        
        chrom = gene_info['chr']
        strand = gene_info['strand']
        key = (chrom, strand)
        index[key].append((transcript_start, transcript_end, gene_info['gene_name']))
    
    # Sort each chromosome/strand list by start position for potential binary search
    for key in index:
        index[key] = sorted(set(index[key]))  # Remove duplicates and sort
    
    total_entries = sum(len(v) for v in index.values())
    logger.info(f"Built index with {len(index)} chromosome/strand combinations, {total_entries} gene regions")
    
    return index


def find_overlapping_genes(intron_coords, gene_index, intron_gene_map=None):
    """
    Find genes that overlap with an intron using spatial index.
    
    Args:
        intron_coords: Tuple of (chr, start, end, strand)
        gene_index: Spatial index dict from build_gene_index()
        intron_gene_map: Optional dict mapping intron coords to gene names (from cache)
        
    Returns:
        List of gene names that overlap the intron
    """
    # If we have cached intron->gene mapping, use it first
    if intron_gene_map and intron_coords in intron_gene_map:
        return sorted(intron_gene_map[intron_coords])
    
    chrom, start, end, strand = intron_coords
    key = (chrom, strand)
    
    # Get genes on this chromosome/strand
    gene_regions = gene_index.get(key, [])
    
    overlapping_genes = set()
    for g_start, g_end, gene_name in gene_regions:
        # Check for overlap: intron overlaps if NOT (intron ends before gene starts OR intron starts after gene ends)
        if not (end < g_start or start > g_end):
            overlapping_genes.add(gene_name)
    
    return sorted(overlapping_genes)


def annotate_introns_with_gtf(integrated_df, gtf_file):
    """
    Annotate introns with gene information from GTF file.
    
    Args:
        integrated_df: Integrated results DataFrame
        gtf_file: Path to GTF annotation file
        
    Returns:
        Annotated DataFrame with additional columns:
        - gene_name: Best matching gene
        - overlapping_genes: All overlapping genes (comma-separated)
        - intron_status: 'known' or 'novel'
    """
    logger.info("Annotating introns with GTF information...")
    
    # Parse GTF (uses cache if available)
    gene_map, annotated_introns, transcript_genes, transcript_exons, intron_gene_map = parse_gtf_file(gtf_file)
    
    # Build spatial index for fast gene lookups
    gene_index = build_gene_index(gene_map, transcript_genes, transcript_exons)
    
    # Annotate each intron
    gene_names = []
    overlapping_genes_list = []
    intron_statuses = []
    
    total_introns = len(integrated_df)
    logger.info(f"Annotating {total_introns} introns...")
    
    # Adaptive progress reporting based on dataset size
    if total_introns < 1000:
        report_every = 100
    elif total_introns < 10000:
        report_every = 1000
    elif total_introns < 100000:
        report_every = 5000
    else:
        report_every = 10000
    
    start_time = time.time()
    last_report_time = start_time
    
    for idx, intron_id in enumerate(integrated_df['intron_id']):
        # Progress indicator with timing
        if (idx + 1) % report_every == 0 or (idx + 1) == total_introns:
            elapsed = time.time() - start_time
            current_time = time.time()
            interval = current_time - last_report_time
            last_report_time = current_time
            
            rate = (idx + 1) / elapsed if elapsed > 0 else 0
            remaining = (total_introns - idx - 1) / rate if rate > 0 else 0
            
            logger.info(
                f"  Processed {idx + 1:,}/{total_introns:,} introns "
                f"({100*(idx+1)/total_introns:.1f}%) | "
                f"Rate: {rate:.0f} introns/sec | "
                f"Elapsed: {elapsed:.1f}s | "
                f"Remaining: ~{remaining:.0f}s"
            )
        
        coords = parse_intron_id(intron_id)
        
        if coords is None:
            gene_names.append('.')
            overlapping_genes_list.append('.')
            intron_statuses.append('unknown')
            continue
        
        # Check if intron is known (exact match)
        is_known = coords in annotated_introns
        intron_statuses.append('known' if is_known else 'novel')
        
        # Find overlapping genes (uses spatial index and cache)
        overlapping = find_overlapping_genes(coords, gene_index, intron_gene_map)
        
        if overlapping:
            gene_names.append(overlapping[0])  # Best match (first one)
            overlapping_genes_list.append(','.join(overlapping))
        else:
            gene_names.append('.')
            overlapping_genes_list.append('.')
    
    # Final timing summary
    total_time = time.time() - start_time
    avg_rate = total_introns / total_time if total_time > 0 else 0
    logger.info(f"Annotation completed in {total_time:.1f}s (average rate: {avg_rate:.0f} introns/sec)")
    
    # Add columns to dataframe (insert right after intron_id)
    integrated_df = integrated_df.copy()
    
    # Get column order - insert annotation columns after intron_id
    cols = list(integrated_df.columns)
    intron_id_idx = cols.index('intron_id')
    
    # Insert new columns after intron_id
    cols.insert(intron_id_idx + 1, 'gene_name')
    cols.insert(intron_id_idx + 2, 'intron_status')
    cols.insert(intron_id_idx + 3, 'overlapping_genes')
    
    # Add the data
    integrated_df['gene_name'] = gene_names
    integrated_df['overlapping_genes'] = overlapping_genes_list
    integrated_df['intron_status'] = intron_statuses
    
    # Reorder columns
    integrated_df = integrated_df[cols]
    
    # Log summary statistics
    known_count = (integrated_df['intron_status'] == 'known').sum()
    novel_count = (integrated_df['intron_status'] == 'novel').sum()
    unknown_count = (integrated_df['intron_status'] == 'unknown').sum()
    
    logger.info(f"Annotation summary:")
    logger.info(f"  Known introns: {known_count} ({100*known_count/len(integrated_df):.1f}%)")
    logger.info(f"  Novel introns: {novel_count} ({100*novel_count/len(integrated_df):.1f}%)")
    if unknown_count > 0:
        logger.info(f"  Unknown (failed parsing): {unknown_count}")
    
    with_genes = (integrated_df['gene_name'] != '.').sum()
    logger.info(f"  Introns with gene assignment: {with_genes} ({100*with_genes/len(integrated_df):.1f}%)")
    
    return integrated_df


def load_results(donor_file, acceptor_file):
    """
    Load donor and acceptor results files.
    
    Args:
        donor_file: Path to donor intron results
        acceptor_file: Path to acceptor intron results
        
    Returns:
        Tuple of (donor_df, acceptor_df)
    """
    logger.info(f"Loading donor results from {donor_file}")
    donor_df = pd.read_csv(donor_file, sep="\t")
    
    logger.info(f"Loading acceptor results from {acceptor_file}")
    acceptor_df = pd.read_csv(acceptor_file, sep="\t")
    
    logger.info(f"Loaded {len(donor_df)} donor introns, {len(acceptor_df)} acceptor introns")
    
    return donor_df, acceptor_df


def integrate_intron_results(donor_df, acceptor_df):
    """
    Integrate donor and acceptor intron-level results.
    
    For large datasets with multiple contrasts, integrates per-contrast to reduce memory usage.
    
    Args:
        donor_df: DataFrame with donor results
        acceptor_df: DataFrame with acceptor results
        
    Returns:
        Integrated DataFrame with columns indicating significance in each analysis
    """
    logger.info("Integrating intron-level results...")
    
    # Check if we have contrast columns (multiple contrasts per intron)
    has_contrasts = 'contrast' in donor_df.columns or 'contrast' in acceptor_df.columns
    
    if has_contrasts:
        # Get unique contrasts from both datasets
        donor_contrasts = set(donor_df['contrast'].unique()) if 'contrast' in donor_df.columns else set()
        acceptor_contrasts = set(acceptor_df['contrast'].unique()) if 'contrast' in acceptor_df.columns else set()
        all_contrasts = sorted(donor_contrasts | acceptor_contrasts)
        
        logger.info(f"Found {len(all_contrasts)} contrasts - integrating per-contrast to reduce memory usage")
        
        # Integrate each contrast separately and combine
        integrated_chunks = []
        for i, contrast in enumerate(all_contrasts, 1):
            if i % 5 == 0 or i == len(all_contrasts):
                logger.info(f"  Processing contrast {i}/{len(all_contrasts)}: {contrast}")
            
            # Filter to single contrast
            donor_subset = donor_df[donor_df['contrast'] == contrast].copy() if 'contrast' in donor_df.columns else pd.DataFrame()
            acceptor_subset = acceptor_df[acceptor_df['contrast'] == contrast].copy() if 'contrast' in acceptor_df.columns else pd.DataFrame()
            
            # Integrate this contrast
            if len(donor_subset) > 0 and len(acceptor_subset) > 0:
                chunk = integrate_single_contrast(donor_subset, acceptor_subset)
            elif len(donor_subset) > 0:
                chunk = integrate_single_contrast(donor_subset, pd.DataFrame())
            elif len(acceptor_subset) > 0:
                chunk = integrate_single_contrast(pd.DataFrame(), acceptor_subset)
            else:
                continue
            
            integrated_chunks.append(chunk)
        
        logger.info(f"Combining {len(integrated_chunks)} integrated contrasts...")
        integrated = pd.concat(integrated_chunks, ignore_index=True)
        logger.info(f"Combined into {len(integrated)} total rows")
    else:
        # Single contrast - integrate directly
        integrated = integrate_single_contrast(donor_df, acceptor_df)
    
    # Summary statistics for combined data
    logger.info(f"\n=== Integration Summary ===")
    logger.info(f"Total unique introns: {len(integrated)}")
    logger.info(f"Tested in both analyses: {(integrated['tested_in'] == 'both').sum()}")
    logger.info(f"Tested in donor only: {(integrated['tested_in'] == 'donor_only').sum()}")
    logger.info(f"Tested in acceptor only: {(integrated['tested_in'] == 'acceptor_only').sum()}")
    
    sig_counts = integrated['significant_in'].value_counts()
    logger.info(f"\nSignificance summary:")
    for status, count in sig_counts.items():
        logger.info(f"  {status}: {count}")
    
    # Direction consistency for those significant in both
    both_sig_count = (integrated['significant_in'] == 'both').sum()
    if both_sig_count > 0:
        both_sig_mask = integrated['significant_in'] == 'both'
        consistent = integrated.loc[both_sig_mask, 'direction_consistent'].sum()
        logger.info(f"\nDirection consistency (significant in both):")
        logger.info(f"  Consistent: {consistent}/{both_sig_count} ({100*consistent/both_sig_count:.1f}%)")
    
    return integrated


def integrate_single_contrast(donor_df, acceptor_df):
    """
    Integrate donor and acceptor results for a single contrast.
    
    Args:
        donor_df: DataFrame with donor results (single contrast)
        acceptor_df: DataFrame with acceptor results (single contrast)
        
    Returns:
        Integrated DataFrame
    """
    
    # Add clustering type indicator
    donor_df = donor_df.copy()
    acceptor_df = acceptor_df.copy()
    
    # Preserve contrast column if it exists
    has_contrast = 'contrast' in donor_df.columns or 'contrast' in acceptor_df.columns
    
    # Rename cluster-specific columns
    donor_df = donor_df.rename(columns={
        'logFC': 'donor_logFC',
        'logCPM': 'donor_logCPM',
        'F': 'donor_F',
        'PValue': 'donor_PValue',
        'FDR': 'donor_FDR',
        'cluster': 'donor_cluster',
        'significant': 'donor_significant',
        'contrast': 'donor_contrast'  # Preserve contrast if present
    })
    
    acceptor_df = acceptor_df.rename(columns={
        'logFC': 'acceptor_logFC',
        'logCPM': 'acceptor_logCPM',
        'F': 'acceptor_F',
        'PValue': 'acceptor_PValue',
        'FDR': 'acceptor_FDR',
        'cluster': 'acceptor_cluster',
        'significant': 'acceptor_significant',
        'contrast': 'acceptor_contrast'  # Preserve contrast if present
    })
    
    # Merge on intron_id (outer join to get all introns)
    integrated = pd.merge(
        donor_df,
        acceptor_df,
        on='intron_id',
        how='outer',
        suffixes=('', '_dup')
    )
    
    # Create summary columns
    integrated['tested_in'] = np.where(
        integrated['donor_PValue'].notna() & integrated['acceptor_PValue'].notna(),
        'both',
        np.where(integrated['donor_PValue'].notna(), 'donor_only', 'acceptor_only')
    )
    
    # Significance status
    donor_sig = integrated['donor_significant'].fillna(False)
    acceptor_sig = integrated['acceptor_significant'].fillna(False)
    donor_sig = donor_sig.astype(bool)
    acceptor_sig = acceptor_sig.astype(bool)
    
    integrated['significant_in'] = np.where(
        donor_sig & acceptor_sig,
        'both',
        np.where(
            donor_sig,
            'donor_only',
            np.where(acceptor_sig, 'acceptor_only', 'neither')
        )
    )
    
    # For introns significant in both, check direction consistency
    both_sig = (donor_sig & acceptor_sig)
    if both_sig.any():
        same_direction = np.sign(integrated.loc[both_sig, 'donor_logFC']) == \
                        np.sign(integrated.loc[both_sig, 'acceptor_logFC'])
        integrated.loc[both_sig, 'direction_consistent'] = same_direction
    else:
        integrated['direction_consistent'] = np.nan
    
    # Best (most significant) analysis
    integrated['best_FDR'] = integrated[['donor_FDR', 'acceptor_FDR']].min(axis=1)
    integrated['best_analysis'] = np.where(
        integrated['donor_FDR'] <= integrated['acceptor_FDR'],
        'donor',
        'acceptor'
    )
    
    # For best analysis, get the corresponding logFC
    integrated['best_logFC'] = np.where(
        integrated['best_analysis'] == 'donor',
        integrated['donor_logFC'],
        integrated['acceptor_logFC']
    )
    
    # Reorder columns for readability
    priority_cols = [
        'intron_id',
        'donor_contrast',
        'acceptor_contrast',
        'gene_name',
        'intron_status',
        'overlapping_genes',
        'tested_in',
        'significant_in',
        'best_analysis',
        'best_FDR',
        'best_logFC',
        'direction_consistent',
        'donor_cluster',
        'donor_logFC',
        'donor_PValue',
        'donor_FDR',
        'donor_significant',
        'acceptor_cluster',
        'acceptor_logFC',
        'acceptor_PValue',
        'acceptor_FDR',
        'acceptor_significant',
    ]
    
    # Keep priority columns first (only if they exist), then any remaining columns
    existing_priority_cols = [col for col in priority_cols if col in integrated.columns]
    other_cols = [col for col in integrated.columns if col not in priority_cols]
    integrated = integrated[existing_priority_cols + other_cols]
    
    # Sort by best FDR
    integrated = integrated.sort_values('best_FDR')
    
    return integrated
    
    # Direction consistency for those significant in both
    both_sig_count = (integrated['significant_in'] == 'both').sum()
    if both_sig_count > 0:
        both_sig_mask = integrated['significant_in'] == 'both'
        consistent_count = (integrated.loc[both_sig_mask, 'direction_consistent'] == True).sum()
        opposite_count = (integrated.loc[both_sig_mask, 'direction_consistent'] == False).sum()
        logger.info(f"\nOf {both_sig_count} introns significant in both:")
        logger.info(f"  Consistent direction: {consistent_count}")
        logger.info(f"  Opposite direction: {opposite_count}")
    
    return integrated


def create_summary_table(integrated_df):
    """
    Create a high-level summary table of integration results.
    
    Args:
        integrated_df: Integrated results DataFrame
        
    Returns:
        Summary DataFrame
    """
    summary = {
        'total_introns': len(integrated_df),
        'tested_both': (integrated_df['tested_in'] == 'both').sum(),
        'tested_donor_only': (integrated_df['tested_in'] == 'donor_only').sum(),
        'tested_acceptor_only': (integrated_df['tested_in'] == 'acceptor_only').sum(),
        'significant_both': (integrated_df['significant_in'] == 'both').sum(),
        'significant_donor_only': (integrated_df['significant_in'] == 'donor_only').sum(),
        'significant_acceptor_only': (integrated_df['significant_in'] == 'acceptor_only').sum(),
        'significant_neither': (integrated_df['significant_in'] == 'neither').sum(),
    }
    
    # Direction consistency for those significant in both
    both_sig = integrated_df[integrated_df['significant_in'] == 'both']
    if len(both_sig) > 0:
        # Count True/False explicitly to avoid issues with NaN and boolean negation
        consistent = (both_sig['direction_consistent'] == True).sum()
        opposite = (both_sig['direction_consistent'] == False).sum()
        summary['both_sig_consistent_direction'] = int(consistent)
        summary['both_sig_opposite_direction'] = int(opposite)
    else:
        summary['both_sig_consistent_direction'] = 0
        summary['both_sig_opposite_direction'] = 0
    
    return pd.DataFrame([summary])


def main():
    parser = argparse.ArgumentParser(
        description="Integrate donor and acceptor differential splicing results",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    
    parser.add_argument(
        "--donor_results",
        type=str,
        required=True,
        help="Donor intron results file (.intron_results.tsv)",
    )
    
    parser.add_argument(
        "--acceptor_results",
        type=str,
        required=True,
        help="Acceptor intron results file (.intron_results.tsv)",
    )
    
    parser.add_argument(
        "--output_prefix",
        type=str,
        required=True,
        help="Output prefix for integrated results",
    )
    
    parser.add_argument(
        "--gtf",
        type=str,
        default=None,
        help="GTF annotation file for gene annotation and known/novel intron status",
    )
    
    parser.add_argument(
        "--fdr_threshold",
        type=float,
        default=0.05,
        help="FDR threshold for filtering significant results",
    )
    
    args = parser.parse_args()
    
    # Load results
    donor_df, acceptor_df = load_results(args.donor_results, args.acceptor_results)
    
    # Integrate
    integrated = integrate_intron_results(donor_df, acceptor_df)
    
    # Annotate with GTF if provided
    if args.gtf:
        integrated = annotate_introns_with_gtf(integrated, args.gtf)
    else:
        logger.info("No GTF file provided, skipping gene annotation")
    
    # Write all integrated results
    all_results_file = f"{args.output_prefix}.integrated_results.tsv"
    logger.info(f"\nWriting all integrated results to {all_results_file}")
    integrated.to_csv(all_results_file, sep="\t", index=False)
    
    # Filter to significant introns (in at least one analysis)
    significant = integrated[integrated['significant_in'] != 'neither']
    
    if len(significant) > 0:
        sig_results_file = f"{args.output_prefix}.significant_integrated.tsv"
        logger.info(f"Writing {len(significant)} significant introns to {sig_results_file}")
        significant.to_csv(sig_results_file, sep="\t", index=False)
        
        # Create summary table
        summary = create_summary_table(integrated)
        summary_file = f"{args.output_prefix}.integration_summary.tsv"
        logger.info(f"Writing integration summary to {summary_file}")
        summary.to_csv(summary_file, sep="\t", index=False)
    else:
        logger.warning("No significant introns found in either analysis")
    
    logger.info("\nIntegration complete!")


if __name__ == "__main__":
    main()
